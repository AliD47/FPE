{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "import json\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna \n",
    "import winsound\n",
    "tf.random.set_seed(47)\n",
    "np.random.seed(47)\n",
    "# import keras_tuner as kt\n",
    "\n",
    "X = pd.read_csv(\"C:/Users/Administrator/Desktop/PFE/data/new/miscplants_Xp.csv\", sep=';')\n",
    "Y = pd.read_csv(\"C:/Users/Administrator/Desktop/PFE/data/new/miscplants_Y.csv\", sep=';')\n",
    "M = pd.read_csv(\"C:/Users/Administrator/Desktop/PFE/data/new/miscplants_M.csv\", sep=';', na_values ='missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(Var):\n",
    "\n",
    "    if Var not in Y.columns or Var not in M.columns:\n",
    "        raise ValueError(f\"Errer Erreur Erreur ! ! !\")\n",
    "\n",
    "    mask = M[Var]\n",
    "\n",
    "    # Split X and Y based on M.csv values\n",
    "    X_cal = X[mask == 'cal']\n",
    "    Y_cal = Y.loc[X_cal.index, Var]\n",
    "\n",
    "    X_val = X[mask == 'val']\n",
    "    Y_val = Y.loc[X_val.index, Var]\n",
    "\n",
    "    X_test = X[mask == 'test']\n",
    "    Y_test = Y.loc[X_test.index, Var]\n",
    "\n",
    "    return (X_cal, Y_cal), (X_val, Y_val), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Var = \"adl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y: (1067,), (356,)\n",
      "X: (1067, 700), (356, 700)\n"
     ]
    }
   ],
   "source": [
    "# Data Splitting\n",
    "(X_cal, Y_cal), (X_val, Y_val), (X_test, Y_test) = split_data(Var)\n",
    "Y_train = pd.concat([Y_cal, Y_val])\n",
    "X_train = pd.concat([X_cal, X_val])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "# X_cal = X_cal.to_numpy()\n",
    "# X_val = X_val.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "num_features = 700  # Spectral features\n",
    "\n",
    "# Standardization (using X_cal statistics)\n",
    "# mean_cal, std_cal = X_cal.mean(), X_cal.std()\n",
    "# X_cal_N = (X_cal - mean_cal) / std_cal\n",
    "# X_val_N = (X_val - mean_cal) / std_cal\n",
    "# Standardization (using X_train statistics)\n",
    "mean_train, std_train = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "# mean_train, std_train = X_train.mean(), X_train.std()\n",
    "X_train_N = (X_train - mean_train) / std_train\n",
    "X_test_N = (X_test - mean_train) / std_train\n",
    "\n",
    "# Reshape for 1D CNN (batch_size, sequence_length, channels) (taille du batch,longueur de la séquence,canaux)\n",
    "# X_cal_f = X_cal_N[..., np.newaxis]  # Shape: (samples, features, 1)\n",
    "# X_val_f = X_val_N[..., np.newaxis]\n",
    "X_train_f = X_train_N[..., np.newaxis]\n",
    "X_test_f = X_test_N[..., np.newaxis]\n",
    "\n",
    "# X_cal_f.shape, X_val_f.shape, X_train_f.shape, X_test_f.shape\n",
    "# Y_cal.shape, Y_val.shape\n",
    "print(f\"Y: {Y_train.shape}, {Y_test.shape}\")\n",
    "print(f\"X: {X_train.shape}, {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # **Modelling** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs Available:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected; please configure CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ###  CNN-R_v1E -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-R_v1E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trial):\n",
    "    # filter_size = trial.suggest_categorical(\"filter_size\", [50, 100, 300, 700])\n",
    "    # padding_type = 'valid' if filter_size == 700 else 'same'\n",
    "    filter_size = 700\n",
    "    padding_type = 'valid'\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 1, 32)\n",
    "\n",
    "    # Dense layers\n",
    "    num_dense_layers = trial.suggest_int(\"num_dense_layers\", 1, 3)\n",
    "    dense_units = [trial.suggest_int(f\"dense_{i}_units\", 8, 128, step=4) for i in range(num_dense_layers)]\n",
    "\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.6, step=0.005) \n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 0.03, log=True) # log=True for logarithmic scale\n",
    "    l2_reg = trial.suggest_float(\"l2_reg\", 0.0, 0.1, step=5e-4) \n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 256, step=16)\n",
    "    initializer = tf.keras.initializers.he_normal(seed=123)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(700,))\n",
    "    x = tf.keras.layers.Reshape((700, 1))(inputs)\n",
    "    x = tf.keras.layers.Conv1D(filters = num_filters, kernel_size=filter_size,\n",
    "                               padding = padding_type,\n",
    "                               kernel_initializer = initializer,\n",
    "                               kernel_regularizer = tf.keras.regularizers.l2(l2_reg))(x)\n",
    "    # print(\"Conv1D output shape:\", x.shape) \n",
    "    x = tf.keras.layers.Activation('elu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    for i, units in enumerate(dense_units):\n",
    "        x = tf.keras.layers.Dense(units, kernel_initializer=initializer,\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)\n",
    "        x = tf.keras.layers.Activation('elu')(x)\n",
    "        if num_dense_layers > 1:\n",
    "            x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, activation=\"linear\", kernel_initializer=initializer)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()]) \n",
    "    \n",
    "    return model, batch_size\n",
    "Modd = \"CNN-R_v1E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #old\n",
    "# def objective(trial):\n",
    "#     model, batch_size = build_model(trial)\n",
    "    \n",
    "#     # Fixed Cal/Val strategy\n",
    "#     es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=5e-3, patience=10, restore_best_weights=True)\n",
    "#     reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10)\n",
    "\n",
    "#     history = model.fit(X_cal_f, Y_cal,\n",
    "#                   validation_data=(X_val_f, Y_val),\n",
    "#                   epochs=300,\n",
    "#                   batch_size=batch_size,\n",
    "#                   callbacks=[es_callback, reduce_lr],\n",
    "#                   verbose=0)\n",
    "#         # Plot losses for the first trial\n",
    "#     if trial.number == 1 or trial.number == 26 or trial.number == 45:\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.plot(history.history['loss'], label='Training Loss')\n",
    "#         plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "#         plt.title(f'Training & Validation Loss - Trial {trial.number}')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.ylabel('MSE Loss')\n",
    "#         plt.legend()\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "#     val_score = model.evaluate(X_val_f, Y_val, verbose=1)\n",
    "#     return val_score[1]  # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    epochs_list = []\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=5e-2, patience=20, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.4, patience=10)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "    rmse_scores = []\n",
    "    history_dicts = []  # to collect all fold histories\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train_f):  \n",
    "        X_train_cv, X_val_cv = X_train_f[train_idx], X_train_f[val_idx]\n",
    "        Y_train_cv, Y_val_cv = Y_train.iloc[train_idx], Y_train.iloc[val_idx]\n",
    "\n",
    "        model, batch_size = build_model(trial)  # rebuild the model for each fold\n",
    "        history = model.fit(X_train_cv, Y_train_cv,\n",
    "                            validation_data=(X_val_cv, Y_val_cv),\n",
    "                            epochs=300,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=[es_callback, reduce_lr],\n",
    "                            verbose=0)\n",
    "        epochs_list.append(len(history.history['loss']))\n",
    "        history_dicts.append(history.history) # store history for each fold\n",
    "\n",
    "\n",
    "        val_loss, val_rmse = model.evaluate(X_val_cv, Y_val_cv, verbose=0)\n",
    "        rmse_scores.append(val_rmse)\n",
    "        avg_epochs = int(np.mean(epochs_list))\n",
    "        trial.set_user_attr(\"avg_epochs\", avg_epochs)\n",
    "        trial.set_user_attr(\"epochs_list\", epochs_list)\n",
    "        trial.set_user_attr(\"fold_histories\", history_dicts)\n",
    "\n",
    "    return np.mean(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Optuna study\n",
    "ii = datetime.now()\n",
    "iii = str(ii.strftime(\"%Y-%m-%d_%Hh%M\"))\n",
    "print(\" **********************************    Start time:\", iii,\"   ***********************************\",\"\\n\")\n",
    "# Set up the Optuna study \n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"/optuna/cnn_hpo\",\n",
    "    # storage=f\"sqlite:///optuna/cnn_study__{iii}.db\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best trial\n",
    "print(\"Best trial :\")\n",
    "print(f\"  RMSE on Val: {study.best_value:.4f}\")\n",
    "print(\"  Best params:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "for i in range(5): winsound.Beep(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs and plots n stuff\n",
    "print(\"Average epochs:\", study.best_trial.user_attrs[\"avg_epochs\"])\n",
    "print(\"Epochs per fold:\", study.best_trial.user_attrs[\"epochs_list\"])\n",
    "\n",
    "# Pick trial indices\n",
    "start_idx = 0\n",
    "middle_idx = len(study.trials) // 2\n",
    "end_idx = len(study.trials)-1\n",
    "chosen_idxs = [start_idx, middle_idx, end_idx]\n",
    "\n",
    "for idx in chosen_idxs:\n",
    "    trial = study.trials[idx]\n",
    "    histories = trial.user_attrs.get(\"fold_histories\", [])\n",
    "\n",
    "    for fold_i, history in enumerate(histories):\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "        # Plot Losses on left axis\n",
    "        ax1.plot(history[\"loss\"], label=\"Training Loss\", color='tab:blue')\n",
    "        ax1.plot(history[\"val_loss\"], label=\"Validation Loss\", color='tab:orange')\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Loss\", color='black')\n",
    "        ax1.tick_params(axis='y', labelcolor='black')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Plot Learning Rate on right axis if available\n",
    "        if \"lr\" in history:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(history[\"lr\"], label=\"Learning Rate\", color='tab:green', linestyle=\"--\", alpha=0.7)\n",
    "            ax2.set_ylabel(\"Learning Rate\", color='tab:green')\n",
    "            ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "\n",
    "        plt.title(f\"Trial {trial.number} - Fold {fold_i + 1}\")\n",
    "        fig.tight_layout()\n",
    "        fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get best params\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "#  Build model function that accepts params directly (not trial object)\n",
    "def build_best_model(params):\n",
    "    filter_size = 700\n",
    "    padding_type = 'valid'\n",
    "    num_filters = params['num_filters']  # or keep as params['num_filters'] if you want\n",
    "\n",
    "    initializer = tf.keras.initializers.he_normal(seed=123)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(700,))\n",
    "    x = tf.keras.layers.Reshape((700, 1))(inputs)\n",
    "    x = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=filter_size,\n",
    "                               padding=padding_type,\n",
    "                               kernel_initializer=initializer,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(params['l2_reg']))(x)\n",
    "    x = tf.keras.layers.Activation('elu')(x)\n",
    "    print(\"Conv1D output shape:\", x.shape)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    for i in range(params['num_dense_layers']):\n",
    "        units = params[f'dense_{i}_units']\n",
    "        x = tf.keras.layers.Dense(units, kernel_initializer=initializer,\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(params['l2_reg']))(x)\n",
    "        x = tf.keras.layers.Activation('elu')(x)\n",
    "        if params['num_dense_layers'] > 1:\n",
    "            x = tf.keras.layers.Dropout(rate=params['dropout_rate'])(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, activation=\"linear\", kernel_initializer=initializer)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=params['learning_rate']),\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #OLD\n",
    "# #  Cross-validation to estimate number of epochs\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "# early_stopping_epochs = []\n",
    "\n",
    "# for train_idx, val_idx in kf.split(X_train_f):\n",
    "#     X_tr, X_v = X_train_f[train_idx], X_train_f[val_idx]\n",
    "#     Y_tr, Y_v = Y_train.iloc[train_idx], Y_train.iloc[val_idx]\n",
    "    \n",
    "#     model = build_best_model(best_params)\n",
    "#     es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, min_delta=5e-3, restore_best_weights=True)\n",
    "    \n",
    "#     history = model.fit(X_tr, Y_tr,\n",
    "#                         validation_data=(X_v, Y_v),\n",
    "#                         epochs=500,\n",
    "#                         batch_size=best_params['batch_size'],\n",
    "#                         callbacks=[es_callback],\n",
    "#                         verbose=0)\n",
    "    \n",
    "#     stopped_epoch = es_callback.stopped_epoch or len(history.history['loss'])\n",
    "#     early_stopping_epochs.append(stopped_epoch)\n",
    "\n",
    "# # 4. Estimate final number of epochs\n",
    "# final_n_epochs = int(np.mean(early_stopping_epochs))\n",
    "# print(\"the number of epochs for each fold:\", early_stopping_epochs)\n",
    "# print(\"Estimated number of epochs for final training:\", final_n_epochs)\n",
    "\n",
    "# for i in range(5): winsound.Beep(500, 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full training set with estimated epochs\n",
    "final_model = build_best_model(best_params)\n",
    "final_model.fit(X_train_f, Y_train,\n",
    "                epochs=study.best_trial.user_attrs[\"avg_epochs\"],\n",
    "                batch_size=best_params['batch_size'],\n",
    "                verbose=0)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_rmse = final_model.evaluate(X_test_f, Y_test, verbose=1)\n",
    "print(f\"RMSE on test set: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modèle</th>\n",
       "      <th>Variable</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RE</th>\n",
       "      <th>RPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN-R_v1D</td>\n",
       "      <td>cp</td>\n",
       "      <td>1.187024</td>\n",
       "      <td>0.104192</td>\n",
       "      <td>3.918738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN-R_v1E</td>\n",
       "      <td>adf</td>\n",
       "      <td>3.389963</td>\n",
       "      <td>0.102010</td>\n",
       "      <td>2.945358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN-R_v1D</td>\n",
       "      <td>adf</td>\n",
       "      <td>3.940099</td>\n",
       "      <td>0.118565</td>\n",
       "      <td>2.534112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN-R_v1E</td>\n",
       "      <td>dmdcell</td>\n",
       "      <td>4.809555</td>\n",
       "      <td>0.090238</td>\n",
       "      <td>3.442970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN-R_v1D</td>\n",
       "      <td>dmdcell</td>\n",
       "      <td>5.119794</td>\n",
       "      <td>0.096058</td>\n",
       "      <td>3.234340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN-R_v1E</td>\n",
       "      <td>adl</td>\n",
       "      <td>2.946611</td>\n",
       "      <td>0.268297</td>\n",
       "      <td>2.635941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CNN-R_v1E</td>\n",
       "      <td>ndf</td>\n",
       "      <td>4.696548</td>\n",
       "      <td>0.089170</td>\n",
       "      <td>2.889557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CNN-R_v1D</td>\n",
       "      <td>ndf</td>\n",
       "      <td>3.938443</td>\n",
       "      <td>0.074776</td>\n",
       "      <td>3.445764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CNN-R_v1E</td>\n",
       "      <td>cf</td>\n",
       "      <td>2.939514</td>\n",
       "      <td>0.102038</td>\n",
       "      <td>3.113246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CNN-R_v1D</td>\n",
       "      <td>cf</td>\n",
       "      <td>2.857829</td>\n",
       "      <td>0.099203</td>\n",
       "      <td>3.202231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CNN-R_v1D</td>\n",
       "      <td>adl</td>\n",
       "      <td>2.606926</td>\n",
       "      <td>0.237368</td>\n",
       "      <td>2.979407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CNN-R_v1E</td>\n",
       "      <td>cp</td>\n",
       "      <td>1.120512</td>\n",
       "      <td>0.098354</td>\n",
       "      <td>4.151350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CNN-R_v1E</td>\n",
       "      <td>adl</td>\n",
       "      <td>2.609518</td>\n",
       "      <td>0.237604</td>\n",
       "      <td>2.976448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Modèle Variable      RMSE        RE       RPD\n",
       "0   CNN-R_v1D       cp  1.187024  0.104192  3.918738\n",
       "1   CNN-R_v1E      adf  3.389963  0.102010  2.945358\n",
       "2   CNN-R_v1D      adf  3.940099  0.118565  2.534112\n",
       "3   CNN-R_v1E  dmdcell  4.809555  0.090238  3.442970\n",
       "4   CNN-R_v1D  dmdcell  5.119794  0.096058  3.234340\n",
       "5   CNN-R_v1E      adl  2.946611  0.268297  2.635941\n",
       "6   CNN-R_v1E      ndf  4.696548  0.089170  2.889557\n",
       "7   CNN-R_v1D      ndf  3.938443  0.074776  3.445764\n",
       "8   CNN-R_v1E       cf  2.939514  0.102038  3.113246\n",
       "9   CNN-R_v1D       cf  2.857829  0.099203  3.202231\n",
       "10  CNN-R_v1D      adl  2.606926  0.237368  2.979407\n",
       "11  CNN-R_v1E       cp  1.120512  0.098354  4.151350\n",
       "12  CNN-R_v1E      adl  2.609518  0.237604  2.976448"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcul des métriques\n",
    "rmse = test_rmse\n",
    "rpd = np.std(Y_test) / rmse\n",
    "relative_error = rmse / np.mean(Y_test)\n",
    "\n",
    "# Préparation de la nouvelle ligne\n",
    "new_row = pd.DataFrame({\n",
    "    \"Modèle\": [Modd],\n",
    "    \"Variable\": [Var],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"RE\": [relative_error],\n",
    "    \"RPD\": [rpd]\n",
    "})\n",
    "\n",
    "# Chemin vers ton fichier CSV\n",
    "csv_path = \"C:/Users/Administrator/Desktop/PFE/otha/results/resultats_CNN.csv\"\n",
    "\n",
    "# Si le fichier existe déjà, on l'ouvre et on ajoute la nouvelle ligne\n",
    "if os.path.exists(csv_path):\n",
    "    existing_results = pd.read_csv(csv_path)\n",
    "    updated_results = pd.concat([existing_results, new_row], ignore_index=True)\n",
    "else:\n",
    "    # Si le fichier n'existe pas encore, on crée un nouveau fichier avec juste cette ligne\n",
    "    updated_results = new_row\n",
    "\n",
    "# Sauvegarde\n",
    "updated_results.to_csv(csv_path, index=False, sep=',')\n",
    "updated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model summary to a text file\n",
    "Models = f\"C:/Users/Administrator/Desktop/PFE/otha/models/Archs.txt\"\n",
    "\n",
    "# Open the file in append mode\n",
    "with open(Models, \"a\") as file:\n",
    "    # Write the header with the model name\n",
    "    file.write(f\" ************************************************************\\n\")\n",
    "    file.write(f\" ******************  Model: {Modd}__{Var}  ******************\\n\")\n",
    "    file.write(f\" ************************************************************\\n\")\n",
    "    model_summary = []\n",
    "    final_model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    file.write(\"\\n\".join(model_summary))\n",
    "    file.write(\"\\n\\n\")  # Add some space between entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best hyperparameters to a JSON file\n",
    "params_path = f\"C:/Users/Administrator/Desktop/PFE/otha/models/best parameters/best_params_{Modd}__{Var}.json\"\n",
    "\n",
    "with open(params_path, \"a\") as param_file:\n",
    "    json.dump(best_params, param_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ###  CNN-R_v1D -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-R_v1E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modd = \"CNN-R_v1D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trial):\n",
    "    # CNN-R_v1D logic\n",
    "    filter_size = trial.suggest_int(\"filter_size\", 5, 200, step=20)\n",
    "    padding_type = 'same'\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 1, 32)\n",
    "\n",
    "    # Only 1 dense layer → no need for loop\n",
    "    dense_units = trial.suggest_int(\"dense_0_units\", 8, 128, step=4)\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 0.03, log=True)\n",
    "    l2_reg = trial.suggest_float(\"l2_reg\", 0.0, 0.1, step=5e-4)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 256, step=16)\n",
    "\n",
    "    initializer = tf.keras.initializers.he_normal(seed=123)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(700,))\n",
    "    x = tf.keras.layers.Reshape((700, 1))(inputs)\n",
    "    x = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=filter_size,\n",
    "                               padding=padding_type,\n",
    "                               kernel_initializer=initializer,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)\n",
    "    # print(\"Conv1D output shape:\", x.shape)\n",
    "    x = tf.keras.layers.Activation('elu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Single dense layer, no dropout\n",
    "    x = tf.keras.layers.Dense(dense_units, kernel_initializer=initializer,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)\n",
    "    x = tf.keras.layers.Activation('elu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, activation=\"linear\", kernel_initializer=initializer)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Optuna study \n",
    "ii = datetime.now()\n",
    "iii = str(ii.strftime(\"%Y-%m-%d_%Hh%M\"))\n",
    "print(\" **********************************    Start time:\", iii,\"   ***********************************\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"cnn_hpo\",\n",
    "    # storage=f\"sqlite:///cnn_study__{iii}.db\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best trial\n",
    "print(\"Best trial :\")\n",
    "print(f\"  RMSE on Val: {study.best_value:.4f}\")\n",
    "print(\"  Best params:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "for i in range(5): winsound.Beep(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs and plots n stuff\n",
    "print(\"Average epochs:\", study.best_trial.user_attrs[\"avg_epochs\"])\n",
    "print(\"Epochs per fold:\", study.best_trial.user_attrs[\"epochs_list\"])\n",
    "\n",
    "# Pick trial indices\n",
    "start_idx = 0\n",
    "middle_idx = len(study.trials) // 2\n",
    "end_idx = len(study.trials) -1\n",
    "chosen_idxs = [start_idx, middle_idx, end_idx]\n",
    "\n",
    "for idx in chosen_idxs:\n",
    "    trial = study.trials[idx]\n",
    "    histories = trial.user_attrs.get(\"fold_histories\", [])\n",
    "\n",
    "    for fold_i, history in enumerate(histories):\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "        # Plot Losses on left axis\n",
    "        ax1.plot(history[\"loss\"], label=\"Training Loss\", color='tab:blue')\n",
    "        ax1.plot(history[\"val_loss\"], label=\"Validation Loss\", color='tab:orange')\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Loss\", color='black')\n",
    "        ax1.tick_params(axis='y', labelcolor='black')\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Plot Learning Rate on right axis if available\n",
    "        if \"lr\" in history:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(history[\"lr\"], label=\"Learning Rate\", color='tab:green', linestyle=\"--\", alpha=0.7)\n",
    "            ax2.set_ylabel(\"Learning Rate\", color='tab:green')\n",
    "            ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "\n",
    "        plt.title(f\"Trial {trial.number} - Fold {fold_i + 1}\")\n",
    "        fig.tight_layout()\n",
    "        fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "\n",
    "def build_best_model(params):\n",
    "    filter_size = params['filter_size']\n",
    "    padding_type = 'same'  # This is what they used in v1D\n",
    "    num_filters = params['num_filters']\n",
    "\n",
    "    initializer = tf.keras.initializers.he_normal(seed=123)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(700,))\n",
    "    x = tf.keras.layers.Reshape((700, 1))(inputs)\n",
    "    x = tf.keras.layers.Conv1D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=filter_size,\n",
    "        padding=padding_type,\n",
    "        kernel_initializer=initializer,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(params['l2_reg'])\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Activation('elu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Only one dense layer, no dropout\n",
    "    units = params['dense_0_units']\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units,\n",
    "        kernel_initializer=initializer,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(params['l2_reg'])\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Activation('elu')(x)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, activation=\"linear\", kernel_initializer=initializer)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=params['learning_rate']),\n",
    "        loss=\"mse\",\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Cross-validation to estimate number of epochs\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "# early_stopping_epochs = []\n",
    "\n",
    "# for train_idx, val_idx in kf.split(X_train_f):\n",
    "#     X_tr, X_val = X_train_f[train_idx], X_train_f[val_idx]\n",
    "#     Y_tr, Y_val = Y_train.iloc[train_idx], Y_train.iloc[val_idx]\n",
    "    \n",
    "#     model = build_best_model(best_params)\n",
    "#     es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, min_delta=5e-3, restore_best_weights=True)\n",
    "    \n",
    "#     history = model.fit(X_tr, Y_tr,\n",
    "#                         validation_data=(X_val, Y_val),\n",
    "#                         epochs=400,\n",
    "#                         batch_size=best_params['batch_size'],\n",
    "#                         callbacks=[es_callback],\n",
    "#                         verbose=0)\n",
    "    \n",
    "#     stopped_epoch = es_callback.stopped_epoch or len(history.history['loss'])\n",
    "#     early_stopping_epochs.append(stopped_epoch)\n",
    "\n",
    "# #  Estimate final number of epochs\n",
    "# final_n_epochs = int(np.mean(early_stopping_epochs))\n",
    "# print(\"the number of epochs for each fold:\", early_stopping_epochs)\n",
    "# print(\"Estimated number of epochs for final training:\", final_n_epochs)\n",
    "\n",
    "# for i in range(5): winsound.Beep(500, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full training set with estimated epochs\n",
    "final_model = build_best_model(best_params)\n",
    "final_model.fit(X_train_f, Y_train,\n",
    "                epochs=study.best_trial.user_attrs[\"avg_epochs\"],\n",
    "                batch_size=best_params['batch_size'],\n",
    "                verbose=0)\n",
    "\n",
    "# 6. Evaluate on test set\n",
    "test_loss, test_rmse = final_model.evaluate(X_test_f, Y_test, verbose=1)\n",
    "print(f\"RMSE on test set: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Enregistrer les Résultats -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "rmse = test_rmse\n",
    "rpd = np.std(Y_test) / rmse\n",
    "relative_error = rmse / np.mean(Y_test)\n",
    "\n",
    "# Préparation de la nouvelle ligne\n",
    "new_row = pd.DataFrame({\n",
    "    \"Modèle\": [Modd],\n",
    "    \"Variable\": [Var],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"RE\": [relative_error],\n",
    "    \"RPD\": [rpd]\n",
    "})\n",
    "\n",
    "# Chemin vers ton fichier CSV\n",
    "csv_path = \"C:/Users/Administrator/Desktop/PFE/otha/results/resultats_CNN.csv\"\n",
    "\n",
    "# Si le fichier existe déjà, on l'ouvre et on ajoute la nouvelle ligne\n",
    "if os.path.exists(csv_path):\n",
    "    existing_results = pd.read_csv(csv_path)\n",
    "    updated_results = pd.concat([existing_results, new_row], ignore_index=True)\n",
    "else:\n",
    "    # Si le fichier n'existe pas encore, on crée un nouveau fichier avec juste cette ligne\n",
    "    updated_results = new_row\n",
    "\n",
    "# Sauvegarde\n",
    "updated_results.to_csv(csv_path, index=False, sep=',')\n",
    "updated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model summary to a text file\n",
    "Models = f\"C:/Users/Administrator/Desktop/PFE/otha/models/archs.txt\"\n",
    "\n",
    "# Open the file in append mode\n",
    "with open(Models, \"a\") as file:\n",
    "    # Write the header with the model name\n",
    "    file.write(f\" ************************************************************\\n\")\n",
    "    file.write(f\" ******************  Model: {Modd}__{Var}  ******************\\n\")\n",
    "    file.write(f\" ************************************************************\\n\")\n",
    "    model_summary = []\n",
    "    final_model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    file.write(\"\\n\".join(model_summary))\n",
    "    file.write(\"\\n\\n\")  # Add some space between entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best hyperparameters to a JSON file\n",
    "params_path = f\"C:/Users/Administrator/Desktop/PFE/otha/models/best parameters/best_params_{Modd}__{Var}.json\"\n",
    "\n",
    "with open(params_path, \"a\") as param_file:\n",
    "    json.dump(best_params, param_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = \"C:/Users/Administrator/Desktop/PFE/otha/results/resultats_CNN.csv\"\n",
    "# dff = pd.read_csv(csv_path)\n",
    "# # Remove duplicate rows (keeping only the first occurrence)\n",
    "# df_unique = dff.drop_duplicates(subset=['Modèle', 'Variable'], keep='last')\n",
    "# # Save the cleaned CSV\n",
    "# df_unique.to_csv(csv_path, index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee = pd.read_csv(csv_path)\n",
    "\n",
    "# ee.rename(columns={\"RMSEP\": \"RMSE\"}, inplace=True)\n",
    "# ee.sort_values(by=\"Modèle\", ascending=True, inplace=True)\n",
    "\n",
    "# df_model_E = ee[ee['Modèle'] == 'CNN-R_v1E']\n",
    "# df_model_D = ee[ee['Modèle'] == 'CNN-R_v1D']\n",
    "\n",
    "# df_model_E.to_csv(\"C:/Users/Administrator/Desktop/PFE/otha/results/results_CNN-R_v1E.csv\", index=False)\n",
    "# df_model_D.to_csv(\"C:/Users/Administrator/Desktop/PFE/otha/results/results_CNN-R_v1D.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
