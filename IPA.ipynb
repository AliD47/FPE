{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069adeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import optuna\n",
    "import random\n",
    "import winsound\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras import Model, initializers\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, Dense, LeakyReLU, Input\n",
    "from IPA_architecture import IPA\n",
    "\n",
    "seed = 47\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "X = pd.read_csv(\"C:/Users/Administrator/Desktop/PFE/data/new/miscplants_Xp.csv\", sep=';')\n",
    "Y = pd.read_csv(\"C:/Users/Administrator/Desktop/PFE/data/new/miscplants_Y.csv\", sep=';')\n",
    "M = pd.read_csv(\"C:/Users/Administrator/Desktop/PFE/data/new/miscplants_M.csv\", sep=';', na_values ='missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f525315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(Var):\n",
    "\n",
    "    if Var not in Y.columns or Var not in M.columns:\n",
    "        raise ValueError(f\"Errer Erreur Erreur ! ! !\")\n",
    "\n",
    "    mask = M[Var]\n",
    "\n",
    "    # Split X and Y based on M.csv values\n",
    "    X_cal = X[mask == 'cal']\n",
    "    Y_cal = Y.loc[X_cal.index, Var]\n",
    "\n",
    "    X_val = X[mask == 'val']\n",
    "    Y_val = Y.loc[X_val.index, Var]\n",
    "\n",
    "    X_test = X[mask == 'test']\n",
    "    Y_test = Y.loc[X_test.index, Var]\n",
    "\n",
    "    return (X_cal, Y_cal), (X_val, Y_val), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "817308d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Var = \"ndf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d9a8b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y: (1147,), (382,)\n",
      "X: (1147, 700, 1), (382, 700, 1)\n"
     ]
    }
   ],
   "source": [
    "# Data Splitting\n",
    "(X_cal, Y_cal), (X_val, Y_val), (X_test, Y_test) = split_data(Var)\n",
    "Y_train = pd.concat([Y_cal, Y_val])\n",
    "X_train = pd.concat([X_cal, X_val])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "# X_cal = X_cal.to_numpy()\n",
    "# X_val = X_val.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "num_features = 700  # Spectral features\n",
    "\n",
    "# Standardization (using X_cal statistics)\n",
    "# mean_cal, std_cal = X_cal.mean(), X_cal.std()\n",
    "# X_cal_N = (X_cal - mean_cal) / std_cal\n",
    "# X_val_N = (X_val - mean_cal) / std_cal\n",
    "\n",
    "# Standardization (using X_train statistics)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "# Transform both training and test sets using the training set's statistics\n",
    "X_train_N = scaler.transform(X_train)\n",
    "X_test_N = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for 1D CNN (batch_size, sequence_length, channels) (taille du batch,longueur de la séquence,canaux)\n",
    "# X_cal_f = X_cal_N[..., np.newaxis]  # Shape: (samples, features, 1)\n",
    "# X_val_f = X_val_N[..., np.newaxis]\n",
    "X_train_f = X_train_N[..., np.newaxis]\n",
    "X_test_f = X_test_N[..., np.newaxis]\n",
    "\n",
    "# X_cal_f.shape, X_val_f.shape, X_train_f.shape, X_test_f.shape\n",
    "# Y_cal.shape, Y_val.shape\n",
    "print(f\"Y: {Y_train.shape}, {Y_test.shape}\")\n",
    "print(f\"X: {X_train_f.shape}, {X_test_f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3799828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs Available:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected; please configure CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa3cf2",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c293569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipa_model = IPA ( seed_value = 47, regularization_factor = .0095 )\n",
    "\n",
    "# ipa_model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#     loss='mse',\n",
    "#     metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "# )\n",
    "Modd = \"IPA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14b0d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(lambda_l2, model):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "        # Only use this model's trainable vars\n",
    "        l2 = tf.add_n([\n",
    "            tf.nn.l2_loss(v)\n",
    "            for v in model.trainable_variables\n",
    "            if 'kernel' in v.name\n",
    "        ])\n",
    "        return mae + lambda_l2 * l2\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.model_selection import KFold\n",
    "    import numpy as np\n",
    "\n",
    "    # 1. Hyperparameter suggestions\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    l2 = trial.suggest_loguniform(\"l2\", 1e-5, 1e-2)\n",
    "\n",
    "    # 2. Learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=1e-3\n",
    "    )\n",
    "\n",
    "    # 3. Cross-validation setup\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    rmse_scores = []\n",
    "    epochs_list = []\n",
    "    history_dicts = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train_f):  # X_train_f = your full calibration set\n",
    "        X_train_cv, X_val_cv = X_train_f[train_idx], X_train_f[val_idx]\n",
    "        Y_train_cv, Y_val_cv = Y_train.iloc[train_idx], Y_train.iloc[val_idx]\n",
    "\n",
    "        # 4. Build model for each fold\n",
    "        ipa_model = IPA(seed_value=seed, regularization_factor=l2)\n",
    "\n",
    "        ipa_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            loss=custom_loss(l2, ipa_model),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "\n",
    "        # 5. Callbacks\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_root_mean_squared_error\", min_delta=5e-2, patience=16, restore_best_weights=True)\n",
    "\n",
    "        # 6. Train\n",
    "        history = ipa_model.fit(\n",
    "            X_train_cv, Y_train_cv,\n",
    "            validation_data=(X_val_cv, Y_val_cv),\n",
    "            epochs=100,\n",
    "            batch_size=16,\n",
    "            callbacks=[es_callback],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # 7. Log metrics\n",
    "        val_loss, val_rmse = ipa_model.evaluate(X_val_cv, Y_val_cv, verbose=0)\n",
    "        rmse_scores.append(val_rmse)\n",
    "        epochs_list.append(len(history.history[\"loss\"]))\n",
    "        history_dicts.append(history.history)\n",
    "\n",
    "    # 8. Record info in trial\n",
    "    avg_epochs = int(np.mean(epochs_list))\n",
    "    trial.set_user_attr(\"avg_epochs\", avg_epochs)\n",
    "    trial.set_user_attr(\"epochs_list\", epochs_list)\n",
    "    trial.set_user_attr(\"fold_histories\", history_dicts)\n",
    "\n",
    "    return np.mean(rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cf06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 03:25:32,527] Trial 0 finished with value: 2.547660970687866 and parameters: {'lr': 0.0006866635737651326, 'l2': 2.5435124712539083e-05}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:28:06,796] Trial 1 finished with value: 2.6163887977600098 and parameters: {'lr': 0.002932053067956513, 'l2': 0.0013128096721692786}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:31:17,266] Trial 2 finished with value: 2.600533103942871 and parameters: {'lr': 0.009730456434055847, 'l2': 0.0011364368178092916}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:36:08,022] Trial 3 finished with value: 3.8494243144989015 and parameters: {'lr': 1.3559844958191087e-05, 'l2': 1.4275912760750544e-05}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:40:29,388] Trial 4 finished with value: 3.94004545211792 and parameters: {'lr': 1.0056181031227177e-05, 'l2': 0.00023403285010985112}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:42:44,727] Trial 5 finished with value: 2.5917168617248536 and parameters: {'lr': 0.0016143409835935423, 'l2': 5.6847035859875887e-05}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:45:32,168] Trial 6 finished with value: 2.601541519165039 and parameters: {'lr': 0.004534893616511579, 'l2': 1.0232940685576949e-05}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:49:45,290] Trial 7 finished with value: 3.0575117588043215 and parameters: {'lr': 5.615653160240997e-05, 'l2': 0.0001537841309002328}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:52:47,310] Trial 8 finished with value: 2.569797086715698 and parameters: {'lr': 0.0016370185606529807, 'l2': 1.447838517044638e-05}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:55:30,108] Trial 9 finished with value: 2.5914127349853517 and parameters: {'lr': 0.0015813729575368965, 'l2': 0.0007035923689290048}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 03:58:38,861] Trial 10 finished with value: 2.5788732528686524 and parameters: {'lr': 0.00028947702329240637, 'l2': 0.006474555799611229}. Best is trial 0 with value: 2.547660970687866.\n",
      "[I 2025-05-20 04:01:54,403] Trial 11 finished with value: 2.514914846420288 and parameters: {'lr': 0.0003624540613247698, 'l2': 3.9878331436182786e-05}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:04:58,698] Trial 12 finished with value: 2.527853536605835 and parameters: {'lr': 0.0003926455367833879, 'l2': 5.4280048808485094e-05}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:08:49,150] Trial 13 finished with value: 2.6132271766662596 and parameters: {'lr': 0.00018031507114151272, 'l2': 6.726877571911463e-05}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:13:08,006] Trial 14 finished with value: 2.8527843952178955 and parameters: {'lr': 9.060681616176687e-05, 'l2': 6.136080482367302e-05}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:16:02,536] Trial 15 finished with value: 2.5312416553497314 and parameters: {'lr': 0.0005363443392930685, 'l2': 0.00011791960849296043}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:20:31,367] Trial 16 finished with value: 3.2182880878448485 and parameters: {'lr': 4.235096706549723e-05, 'l2': 3.291705932309868e-05}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:24:21,592] Trial 17 finished with value: 2.666701555252075 and parameters: {'lr': 0.00015498175608757409, 'l2': 0.0003930141331303148}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:27:26,502] Trial 18 finished with value: 2.5350146293640137 and parameters: {'lr': 0.0005373276518641722, 'l2': 3.829495245226245e-05}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:32:15,647] Trial 19 finished with value: 3.468427276611328 and parameters: {'lr': 2.635476818047098e-05, 'l2': 0.00010859963773905108}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:36:01,492] Trial 20 finished with value: 2.5608725547790527 and parameters: {'lr': 0.00026667910842955616, 'l2': 0.0031400259750206504}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:39:00,134] Trial 21 finished with value: 2.526009225845337 and parameters: {'lr': 0.0006056587561431298, 'l2': 0.0001210927452433103}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:41:59,827] Trial 22 finished with value: 2.539953422546387 and parameters: {'lr': 0.0007527497859601002, 'l2': 0.0002938723327885377}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:46:17,629] Trial 23 finished with value: 2.703837442398071 and parameters: {'lr': 0.0001259866888246848, 'l2': 2.7691450872832152e-05}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:49:21,229] Trial 24 finished with value: 2.5619533538818358 and parameters: {'lr': 0.001127790308893264, 'l2': 9.468417289970112e-05}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:53:05,606] Trial 25 finished with value: 2.524410772323608 and parameters: {'lr': 0.0003457696155830931, 'l2': 0.00019120921624660978}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:56:16,624] Trial 26 finished with value: 2.5670135498046873 and parameters: {'lr': 0.0002889820187485502, 'l2': 0.00020154053536981998}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 04:58:33,961] Trial 27 finished with value: 2.5801655292510985 and parameters: {'lr': 0.0009690745925442629, 'l2': 0.0004349567807702768}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 05:02:01,010] Trial 28 finished with value: 2.8377088069915772 and parameters: {'lr': 9.126893844618608e-05, 'l2': 0.0006505802606449558}. Best is trial 11 with value: 2.514914846420288.\n",
      "[I 2025-05-20 05:04:12,806] Trial 29 finished with value: 2.5992934703826904 and parameters: {'lr': 0.0027164139146643607, 'l2': 1.9074022317223733e-05}. Best is trial 11 with value: 2.514914846420288.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial :\n",
      "  RMSE on Val: 2.5149\n",
      "  Best params:\n",
      "    lr: 0.0003624540613247698\n",
      "    l2: 3.9878331436182786e-05\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Print the best trial\n",
    "print(\"Best trial :\")\n",
    "print(f\"  RMSE on Val: {study.best_value:.4f}\")\n",
    "print(\"  Best params:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "for i in range(5): winsound.Beep(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff0c3177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 2s 12ms/step - loss: 13.9833 - root_mean_squared_error: 19.2128\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 7.2994 - root_mean_squared_error: 9.3356\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 5.3592 - root_mean_squared_error: 6.8677\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 4.0484 - root_mean_squared_error: 5.4152\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 3.7538 - root_mean_squared_error: 4.8679\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 3.6558 - root_mean_squared_error: 4.7814\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 3.5979 - root_mean_squared_error: 4.6680\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 3.1523 - root_mean_squared_error: 4.1093\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 3.0131 - root_mean_squared_error: 3.9557\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.8259 - root_mean_squared_error: 3.7267\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.7630 - root_mean_squared_error: 3.7039\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.6366 - root_mean_squared_error: 3.5291\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.7665 - root_mean_squared_error: 3.6164\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.8007 - root_mean_squared_error: 3.6526\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.5018 - root_mean_squared_error: 3.3161\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.5109 - root_mean_squared_error: 3.3396\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.4428 - root_mean_squared_error: 3.2442\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.4136 - root_mean_squared_error: 3.1888\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.2555 - root_mean_squared_error: 3.0557\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.3518 - root_mean_squared_error: 3.1595\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.1929 - root_mean_squared_error: 2.9110\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.1527 - root_mean_squared_error: 2.9249\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.1146 - root_mean_squared_error: 2.8475\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 2.1329 - root_mean_squared_error: 2.8675\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 2.1158 - root_mean_squared_error: 2.8548\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.0059 - root_mean_squared_error: 2.7566\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.0319 - root_mean_squared_error: 2.7415\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.1209 - root_mean_squared_error: 2.8538\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.9248 - root_mean_squared_error: 2.6669\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 2.0915 - root_mean_squared_error: 2.7395\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.9622 - root_mean_squared_error: 2.6597\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 1.9181 - root_mean_squared_error: 2.6098\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.9275 - root_mean_squared_error: 2.6111\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.8867 - root_mean_squared_error: 2.5676\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.8827 - root_mean_squared_error: 2.6092\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.8135 - root_mean_squared_error: 2.4996\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7561 - root_mean_squared_error: 2.4603\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7276 - root_mean_squared_error: 2.3954\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.6955 - root_mean_squared_error: 2.3541\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.6501 - root_mean_squared_error: 2.3059\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7537 - root_mean_squared_error: 2.4406\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7121 - root_mean_squared_error: 2.3626\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7060 - root_mean_squared_error: 2.3303\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.6934 - root_mean_squared_error: 2.3816\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7077 - root_mean_squared_error: 2.3315\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.8324 - root_mean_squared_error: 3.7735\n",
      "RMSE on test set: 3.7735\n"
     ]
    }
   ],
   "source": [
    "# train the final model\n",
    "best_params = {\n",
    "    \"lr\": 0.001,\n",
    "    \"l2\": 0.0001\n",
    "}\n",
    "\n",
    "best_lr = best_params[\"lr\"]\n",
    "best_l2 = best_params[\"l2\"]\n",
    "final_model = IPA(seed_value=47, regularization_factor=best_l2)\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=best_lr,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=1e-3\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "final_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=custom_loss(best_l2, final_model),\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "history = final_model.fit(\n",
    "    X_train_f, Y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_rmse = final_model.evaluate(X_test_f, Y_test, verbose=1)\n",
    "print(f\"RMSE on test set: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87822d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modèle</th>\n",
       "      <th>Variable</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RE</th>\n",
       "      <th>RPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPA</td>\n",
       "      <td>adf</td>\n",
       "      <td>3.564828</td>\n",
       "      <td>0.107272</td>\n",
       "      <td>2.800879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPA</td>\n",
       "      <td>adl</td>\n",
       "      <td>2.905515</td>\n",
       "      <td>0.264555</td>\n",
       "      <td>2.673225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IPA</td>\n",
       "      <td>cf</td>\n",
       "      <td>2.936756</td>\n",
       "      <td>0.101943</td>\n",
       "      <td>3.116170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IPA</td>\n",
       "      <td>cp</td>\n",
       "      <td>1.112475</td>\n",
       "      <td>0.097648</td>\n",
       "      <td>4.181340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IPA</td>\n",
       "      <td>dmdcell</td>\n",
       "      <td>4.909425</td>\n",
       "      <td>0.092111</td>\n",
       "      <td>3.372931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IPA</td>\n",
       "      <td>ndf</td>\n",
       "      <td>3.773490</td>\n",
       "      <td>0.071644</td>\n",
       "      <td>3.596390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Modèle Variable      RMSE        RE       RPD\n",
       "0    IPA      adf  3.564828  0.107272  2.800879\n",
       "1    IPA      adl  2.905515  0.264555  2.673225\n",
       "2    IPA       cf  2.936756  0.101943  3.116170\n",
       "3    IPA       cp  1.112475  0.097648  4.181340\n",
       "4    IPA  dmdcell  4.909425  0.092111  3.372931\n",
       "5    IPA      ndf  3.773490  0.071644  3.596390"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcul des métriques\n",
    "rmse = test_rmse\n",
    "rpd = np.std(Y_test) / rmse\n",
    "relative_error = rmse / np.mean(Y_test)\n",
    "\n",
    "# Préparation de la nouvelle ligne\n",
    "new_row = pd.DataFrame({\n",
    "    \"Modèle\": [Modd],\n",
    "    \"Variable\": [Var],\n",
    "    \"RMSE\": [rmse],\n",
    "    \"RE\": [relative_error],\n",
    "    \"RPD\": [rpd]\n",
    "})\n",
    "\n",
    "# Chemin vers ton fichier CSV\n",
    "csv_path = \"C:/Users/Administrator/Desktop/PFE/otha/results/resultats_IPA_default.csv\"\n",
    "\n",
    "# Si le fichier existe déjà, on l'ouvre et on ajoute la nouvelle ligne\n",
    "if os.path.exists(csv_path):\n",
    "    existing_results = pd.read_csv(csv_path)\n",
    "    updated_results = pd.concat([existing_results, new_row], ignore_index=True)\n",
    "else:\n",
    "    # Si le fichier n'existe pas encore, on crée un nouveau fichier avec juste cette ligne\n",
    "    updated_results = new_row\n",
    "\n",
    "# Sauvegarde\n",
    "updated_results.to_csv(csv_path, index=False, sep=',')\n",
    "updated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = \"C:/Users/Administrator/Desktop/PFE/otha/results/resultats_IPA.csv\"\n",
    "# dff = pd.read_csv(csv_path)\n",
    "# # Remove duplicate rows (keeping only the first occurrence)\n",
    "# df_unique = dff.drop_duplicates(subset=['Modèle', 'Variable'], keep='last')\n",
    "# # Save the cleaned CSV\n",
    "# df_unique.to_csv(csv_path, index=False, sep=',')\n",
    "# df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef089ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model summary to a text file\n",
    "Models = f\"C:/Users/Administrator/Desktop/PFE/otha/models/Archs_.txt\"\n",
    "final_model.save_weights(f\"ipa_{Modd}.h5\")\n",
    "\n",
    "# Open the file in append mode\n",
    "with open(Models, \"a\") as file:\n",
    "    # Write the header with the model name\n",
    "    file.write(f\" ************************************************************\\n\")\n",
    "    file.write(f\" ******************  Model: {Modd}__{Var}  ******************\\n\")\n",
    "    file.write(f\" ************************************************************\\n\")\n",
    "    model_summary = []\n",
    "    final_model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    file.write(\"\\n\".join(model_summary))\n",
    "    file.write(\"\\n\\n\")  # Add some space between entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best hyperparameters to a JSON file\n",
    "params_path = f\"C:/Users/Administrator/Desktop/PFE/otha/models/best parameters/best_params_{Modd}__{Var}.json\"\n",
    "\n",
    "with open(params_path, \"a\") as param_file:\n",
    "    json.dump(best_params, param_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1b2c3",
   "metadata": {},
   "source": [
    "# ________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
